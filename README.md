# 大模型视频知识提取

## 项目简介
本项目旨在通过使用大模型进行视频知识提取，帮助用户从视频中自动提取关键信息，构建背景知识，并通过问答系统提供智能的知识服务。

该系统的主要目标是自动化地从视频中提取字幕并转化为可以进行语义查询的知识结构。

## 项目流程
0. 制作项目的动机
	1. 懒
	2. 学生自己解决问题能力
	3. 偏应用向
1. FFmepg提取视频中的音频部分，返回numpy数组
	1. 图像部分对于知识类视频总结表现不好
	2. 图像部分处理过于复杂，提高处理时间
2. 检测音频中的语音部分VAD（VAD：Voice Activity Detection）
	1. 检测语音段 切割音频
	2. 移除较短语音段
	3. 扩展语音段 （保证头尾语音信息完整）
	4. 合并相邻语音段
3. 使用Whisper模型 语音转文本 返回文本列表
	1. 端到端模型：音频 -> 文本（单一深度神经网络）区别于多阶段处理过程，比ASR系统更高效，避免模型复杂。
	2. 特征提取：DNN提取特征。CNN？
	3. 自注意力机制：Whisper 中的编码器和解码器都基于Transformer模型。Self-Attention机制处理每个输入的特征时，不仅仅依赖于当前时间步的输入，还能关注输入序列中其他位置的信息。
	4. 多任务学习：同时学习不同任务（语音翻译 音频分类）中的共享特征，防止过拟合。
	5. 选择理由：综合”中文识别精度“ ”多语言泛用性“ ”延时（本地部署更快）“
4. 保存字幕流文本为srt md格式
5. 数据清洗：对字幕流文本 的时间戳等处理
6. 字幕流内容转为langchain的Document对象 
	1. 读取字幕文件，每行作为一个独立文本
	2. 独立文本按照指定size合并为块封装为Document对象，便于后续索引和检索
		- 文本分段：如果一次性处理整个字幕段落的所有文字，通常会超出单次会话的字数限制。
			1. 朴素分割：分割速度快 同意话题可能分到不同块
			2. 朴素扩展分割：2者之间 实际表现和朴素相近 
			3. 语义滑动窗口分割：语义关联强 复杂度高速度慢
7. Embedding 将文本转化为高维向量表示
	1. SentenceTransformer 创建嵌入模型（Sentence-BERT）(all-MiniLM-L6-v2)
	2. 对每个Document生成其对应的嵌入向量
8. RAG检索增强生成：构建本地向量数据库
	-  使用 FAISS 库构建一个基于 L2 距离的索引，以便快速检索
		1. 根据嵌入向量维度创建 L2 距离的平面索引
		2. 将所有Document的嵌入向量添加到索引中
		3. 创建一个字典存储每个文档，便于检索后返回具体的文档内容
		4. 使用 FAISS 向量数据库创建检索器，便于后续查询
	 - **检索（Retrieval）**：从一个大的文档库中找到与用户问题最相关的文档。
	 - **生成（Generation）**：将检索到的信息与用户的查询一起，传递给生成模型，生成最终的回答。
9. 提供query，转为嵌入向量后，在faiss数据库中执行相似度查询相似若干Document
10. 使用LangChain与大模型针对视频内容进行QA回答
	1. Models：使用商用API调用模型（个人使用跑本地模型成本大，速度慢）
	2. Agent：无
	3. Chain：无
	4. Indexes：无
	5. Memory：上下文（embedding相似文本）
	6. Prompt：提示模板（给出视频知识大纲）

## 运行顺序
0. 先配置auto-cut的环境
1. 命令行方式生成字幕文件
2. 依次运行srt_to_text.py chat_rag.py (中间可以修改参数，记得用自己的apikey)
3. 其中llm_video和simple_video是两个开发过程测试代码，无需在意

## 项目参考
项目流程前半部分中ffmpeg和Whisper部分参考了[AutoCut]（详情看readme1.md）项目的流程。
